# DASC7606 Final - CNN & CV - summerized by hyperloop

# Section 0 Exam Scope
![](./Scope.png)

# Section 1 Convolutional layer

## 1.1 CONV Layer
![1764760108217](image/Revision-CNN/1764760108217.png)

### 1. 基本概念
- **卷积层（CONV Layer）**：用于计算与输入图像局部连续区域（局部感受野）相连的神经元的输出。
- **局部感受野（LRF）**：神经元“查看”或连接的输入图像的一部分。
- **滤波器（Filter）/卷积核（Kernel）**：一个权重矩阵，用于从LRF中提取特征。

### 2. 工作原理
- **扫描方式**：从左到右、从上到下遍历整个输入图像。
- **计算输出**：
  - 输出 = \( w^T x + b \)
  - 公式：\( \sum_{ij} x_{ij} w_{ij} + b \)，其中 \( x_{ij} \) 是输入值，\( w_{ij} \) 是权重。

### 3. 参数共享
- **参数共享**：在整个扫描过程中，所有LRF使用相同的权重矩阵 \( w \) 和偏置 \( b \)。

### 4. 输出结果
- **特征图（Feature Map）**：每个输出值构成隐藏层（特征图）中的一个元素，表示从输入中提取的特定特征。


### 卷积神经网络基础知识点

### 1. 感受野（Receptive Field）
- **定义**：输入图像中影响特征图上单个像素值的区域大小
- **作用**：决定卷积层能看到输入图像的局部范围大小
- **特点**：层数越深，感受野越大

### 2. 步长（Stride）
- **定义**：卷积核在输入图像上每次移动的像素距离
- **作用**：控制输出特征图的尺寸和下采样程度
- **影响**：
  - 步长=1：输出尺寸较大，信息保留完整
  - 步长>1：输出尺寸减小，具有下采样效果

### 3. 填充（Padding）
- **定义**：在输入图像边缘添加额外像素（通常为0）
- **目的**：
  - **保持尺寸**：使输出特征图与输入尺寸相同
  - **保护边缘信息**：避免边缘信息丢失
- **常见类型**：
  - **Valid Padding**：无填充，输出尺寸变小
  - **Same Padding**：填充使输出尺寸与输入相同

---

## 1.2 Padding
![1764760275582](image/Revision-CNN/1764760275582.png)
## 填充（Padding）类型

### 1. 填充的目的
- 避免每增加一层卷积层，图像尺寸就减少的问题
- 从而允许构建更深层的网络结构

### 2. 填充的常见类型
- **有效卷积（Valid Convolution）**：
  - 不使用填充
  - 存在问题：图像尺寸会逐层减小，限制了网络深度

- **相同卷积（Same Convolution）**：
  - 选择合适的填充值P，使输出图像尺寸与输入保持一致

- **完全卷积（Full Convolution）**：
  - 选择填充值P，使得每个像素都能被k个局部感受野（LRF）包含
  - 通常用于解决边缘像素在卷积过程中代表性不足的问题

### 3. 实用建议
- 最优填充值通常介于"有效卷积"和"相同卷积"之间
- 需要根据具体任务和网络结构权衡选择

![1764760839784](image/Revision-CNN/1764760839784.png)

---

## 1.3 Convolutions
![1764762064230](image/Revision-CNN/1764762064230.png)

### 平移不变性 (Translation Invariance)
- **相同滤波器参数**（局部权重组）可以在图像的**任何位置**识别相同模式
- **训练优势**：只需较少训练样本就能学习到有效特征
- **实际意义**：无论目标出现在图像哪个位置，都能被检测到

### 层级模式学习
卷积神经网络能够学习**层次化的特征模式**：

#### 1. **低层特征**
- 学习**局部基础模式**
- 例如：线条、边缘、角点、简单纹理

#### 2. **中层特征**
- **更大的感受野**（LRF：Local Receptive Field）
- 学习**复杂组合模式**
- 例如：圆形+线条（组成简单物体部分）

#### 3. **高层特征**
- 学习**更复杂的语义特征**
- 例如：车轮、引擎盖、窗户等完整物体部件

**特征学习流程**：
低层特征 → 中层特征 → 高层特征 → 可训练分类器

### 参数更少
与全连接层相比，卷积层具有显著的**参数效率优势**：
- **参数共享**：同一滤波器在整个图像上重复使用
- **局部连接**：每个神经元只连接输入的一小部分区域
- **结果**：大幅减少模型参数量，降低过拟合风险

### 核心优势总结
1. **参数效率高**：比全连接网络参数少得多
2. **平移不变性**：特征检测位置无关
3. **层次化学习**：从简单到复杂的特征抽象
4. **适合视觉任务**：模仿人类视觉系统的处理方式


![1764762041967](image/Revision-CNN/1764762041967.png)


### 为什么在CNN中使用ReLU激活函数？

#### 1. **计算效率高**
- ReLU函数简单：\( f(x) = \max(0, x) \)
- 计算速度快，适合大规模神经网络

#### 2. **训练更快**
- 解决梯度消失问题（在正区域梯度恒为1）
- 相比sigmoid/tanh，梯度更稳定

#### 3. **图像处理特性**
- **卷积操作**：平滑图像中的边界和颜色变化（黑色、灰色、白色的过渡）
- **ReLU整流**：使颜色变化更加突然和明显
  - 负值归零，增强特征对比度
  - 突出重要特征，抑制不相关信息


---

# Section 2 Pooling layer

![1764762478648](image/Revision-CNN/1764762478648.png)

### Why Pooling（为什么要池化）？

- **核心作用**：用“降低分辨率”来**减少参数/计算量**，代价是**丢失精确位置信息**（把语义相近的局部特征合并）。
- **典型设置**：`2×2 pooling, stride=2`
  - 在大致保留位置近似的同时，把特征图的 **宽高各缩小一半**  
  - 因而整体元素数约变为原来的 **1/4**（常说“参数/计算规模减少约 4 倍”）

- **参数更少带来的好处**
  - 更少内存（less memory）
  - 更高效率（more efficient）
  - 更不容易过拟合（less overfitting）

- **对后续卷积层的意义**
  - 让后续卷积在“原图尺度”上等效看到**更大的局部感受野**（LRF / receptive field），从而学习更高层、更复杂的特征

- **平移不变性（translation invariance）**
  - 目标在图中移动一些位置，仍然容易被检测到（例如猫移动了，仍能识别为猫）

> 备注：池化本身不一定“让 LRF 简单翻倍”，但它通过下采样让后续层在输入空间的等效覆盖范围更大。

---


# Section 3 Dense layer

![1764762643696](image/Revision-CNN/1764762643696.png)

### 全连接层（Dense / Fully Connected）

- **位置**：通常位于 CNN 的末尾，用于把卷积/池化提取到的特征用于最终预测（分类/回归）。
- **为什么需要 Dense？**
  - 前面卷积层保留了**空间结构**并学到局部/组合特征；
  - 最后需要把这些特征**聚合成一个整体决策**，Dense 擅长做全局加权组合与判别。

- **Flatten（展平）是什么？**
  - 将最后一层特征张量从 **W × H × D** 转为长度为 **W·H·D** 的一维向量（作为 Dense 的输入）。

- **Flatten 的影响**
  - **缺点**：会丢失特征的具体位置信息（空间关系被打散）。
  - **直觉**：分类很多时候依赖“**是否出现关键部件**”而非精确位置  
    （例如检测到猫的头/尾/腿等特征，整体很可能就是“猫”）。


---

# Section 4 Computation
![1764763303942](image/Revision-CNN/1764763303942.png)
![1764763315460](image/Revision-CNN/1764763315460.png)
![1764763329122](image/Revision-CNN/1764763329122.png)
![1764763347496](image/Revision-CNN/1764763347496.png)
![1764763358575](image/Revision-CNN/1764763358575.png)
![1764763368371](image/Revision-CNN/1764763368371.png)

![1764764670407](image/Revision-CNN/1764764670407.png)
r 是感受野大小、k 是卷积核（或池化窗口）边长、
j-jump这一层的输出相邻位置，映射回原始输入图像时，中心点之间相隔多少像素。

---

# Section 5 Visualizing and Understanding CNN
## 5.1 Probability Heat Maps
![1764766071769](image/Revision-CNN/1764766071769.png)


### 概率热力图 / 显著性图（Probability Heat Maps / Saliency Maps）

- **目的**：找出图像中哪些区域对模型预测最关键（哪些部分“最影响分类结果”）。
- **基本方法（遮挡法 Occlusion）**：
  1. 用一个灰色小块（patch）遮住输入图像的一部分；
  2. 让遮挡块在图像上滑动，多次重复推理；
  3. 记录每次遮挡后，模型对**正确类别**的预测概率变化。
- **像素/区域重要性计算**：
  - 当某个像素/区域被遮挡后，**正确分类概率下降越多** ⇒ 该区域越重要。
- **解释热力图颜色**：
  - **越暗**表示遮挡该区域时正确概率越低（下降越明显）⇒ 该区域对预测越关键。

---


## 5.2 Gradient Ascent 梯度上升做可视化
![1764766256788](image/Revision-CNN/1764766256788.png)
- **目标**：固定 CNN 的所有权重不变，直接“生成一张输入图像” \(I\)，使某个神经元/某个类别的得分最大（看模型最想看到什么图案）。
- **优化目标示例**：最大化类别 \(c\) 的打分（softmax 之前的 logit）并加正则防止图像发散、过噪：
  \[
  \arg\max_I \; S_c(I) - \lambda \lVert I \rVert_2^2
  \]
  - \(S_c(I)\)：类别 \(c\) 的 score（softmax 前）
  - \(\lambda \lVert I \rVert_2^2\)：L2 正则，让图像更平滑/幅度更小

- **初始化**：把输入图像 \(I\) 初始化为全 0（或随机噪声）。

- **迭代步骤（重复多次）**：
  1. **前向传播**：计算当前图像的得分/激活值
  2. **反向传播**：计算目标对图像像素的梯度 \(\nabla_I S_c(I)\)
  3. **更新图像（梯度上升）**：沿梯度方向小步更新，让目标变大  
     \[
     I \leftarrow I + \eta \cdot \nabla_I S_c(I)
     \]
     （同时会被正则项抑制过大幅度）

- **得到的结果**：最终生成的 \(I\) 展示了该神经元/类别“偏好”的输入模式（常用于特征可视化/模型解释）。

---

## 5.3 Maximally Activating Patches 最大激活图像块

![1764766427688](image/Revision-CNN/1764766427688.png)


- **目的**：理解某个神经元“最喜欢什么输入模式”（它对哪些局部图案响应最强）。
- **做法**：
  1. 选定一个神经元（某层某通道/某位置）。
  2. 将大量图片输入网络，**记录该神经元的激活值**。
  3. 取激活值最大的 **Top-k 图片**。
  4. 展示这些图片中对应的 **局部区域 patch（或该神经元的感受野 LRF 区域）**。

- **如何读图**：
  - 每一行对应一个神经元；该行展示让它激活最大的 Top-k patches。
  - **更高层**的神经元通常对应**更大的感受野（LRF 更大）**，因此它们偏好更“整体/语义化”的模式（右图趋势）。

---

![1764767688054](image/Revision-CNN/1764767688054.png)
![1764767700237](image/Revision-CNN/1764767700237.png)

---