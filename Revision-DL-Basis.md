# DASC7606 Final - Deep Learning Basis - summarized by hyperloop

# Section 0 Exam Scope
![](./Scope.png)

# Section 1 Cost Function
![1764756545232](image/Revision-DL-Basis/1764756545232.png)
![1764756577263](image/Revision-DL-Basis/1764756577263.png)

## 常见损失函数对比

| 损失函数 | 公式 | 主要用途 | 特点 | 适用场景 |
|---------|------|---------|------|---------|
| **均方误差 (MSE)** | `(1/n)∑(y - ŷ)²` | 回归问题 | 1. 对异常值敏感<br>2. 处处可导<br>3. 惩罚大误差更重 | 线性回归、数值预测 |
| **平均绝对误差 (MAE)** | `(1/n)∑\|y - ŷ\|` | 回归问题 | 1. 对异常值稳健<br>2. 在零点不可导<br>3. 梯度恒定 | 稳健回归、金融预测 |
| **交叉熵损失** | `-∑y·log(ŷ)` | 分类问题 | 1. 衡量概率分布差异<br>2. 梯度与误差成正比<br>3. 避免梯度饱和 | 逻辑回归、神经网络分类 |
| **Huber损失** | `{0.5δ² if \|δ\|≤ε, ε(\|δ\|-0.5ε) otherwise}` | 回归问题 | 1. MSE和MAE的折衷<br>2. 对异常值适度敏感<br>3. 处处可导 | 稳健回归、离群数据处理 |
| **铰链损失 (Hinge)** | `max(0, 1 - y·ŷ)` | 分类问题 | 1. 用于最大间隔分类<br>2. 仅关注分类边界<br>3. 产生稀疏解 | 支持向量机(SVM) |
| **对数损失** | `-logP(y\|x)` | 概率预测 | 1. 惩罚错误分类的概率<br>2. 鼓励高置信度正确预测 | 概率模型、多分类 |

## 关键选择因素

1. **问题类型**：
   - 回归 → MSE、MAE、Huber
   - 二分类 → 交叉熵、铰链损失
   - 多分类 → 交叉熵

2. **异常值处理**：
   - 异常值多 → MAE、Huber
   - 异常值少 → MSE

3. **梯度特性**：
   - 需要平滑梯度 → MSE、交叉熵
   - 可接受非连续梯度 → MAE

4. **计算效率**：
   - MSE计算最快
   - Huber计算较复杂

---

# Section 2 Gradient Descent

## 基本思想
通过**迭代更新**参数，沿着损失函数梯度的**反方向**移动，逐步找到损失函数的**最小值**。

## 核心公式
参数更新：`θ = θ - η·∇J(θ)`
- `θ`: 模型参数
- `η`: 学习率 (Learning Rate)
- `∇J(θ)`: 损失函数关于θ的梯度


![1764757368494](image/Revision-DL-Basis/1764757368494.png)

## 核心目标
最小化损失函数：  
\[ f(W) = \frac{1}{M} \sum_{i=1}^M e(h(x_i), y_i) \]

其中：
- \(W\) 为模型参数（权重）
- \(M\) 为训练样本总数
- \(e(h(x_i), y_i)\) 表示模型在样本 \((x_i, y_i)\) 上的损失
- \(h(x_i)\) 为模型预测值

## 优化方法
沿损失函数的负梯度方向迭代更新参数：  
\[ \Delta W = -\alpha \nabla f(W) \]

## 关键特点
- **全量计算**：每一步更新都基于 **全部 M 个训练样本** 计算梯度
- **批量处理**：使用整个数据集进行一次参数更新
- **学习率**：\(\alpha\) 控制更新步长，影响收敛速度与稳定性

## 执行流程
1. 初始化权重 \(W\)
2. 计算损失函数在所有训练样本上的梯度 \(\nabla f(W)\)
3. 按学习率 \(\alpha\) 沿负梯度方向更新权重
4. 重复步骤 2-3 直至收敛

## 适用场景
- 训练集规模较小
- 对收敛稳定性要求较高
- 可接受较慢的训练速度

## 注意事项
- 计算开销随数据量线性增长
- 容易陷入局部最优（对于非凸问题）
- 学习率选择对收敛影响显著

![1764757381151](image/Revision-DL-Basis/1764757381151.png)
![1764757397543](image/Revision-DL-Basis/1764757397543.png)

- 不同于基于单个随机样本更新权重的方法，此处使用 **一批（batch）** 随机样本
- 优势：
  - 梯度估计 **更准确**
  - 收敛过程 **更平稳**
  - 小批量训练 **速度更快**！
  - 可并行计算，在 GPU 上实现加速

![1764757633120](image/Revision-DL-Basis/1764757633120.png)


| 维度 | GD | SGD | Mini-batch GD |
|-----|----|-----|---------------|
| **梯度计算** | 全数据集 | 单样本 | 小批量 |
| **收敛速度** | 慢（每次迭代） | 快（每次迭代） | 中等 |
| **收敛稳定性** | 最稳定 | 最不稳定 | 较稳定 |
| **内存需求** | 最高 | 最低 | 中等 |
| **计算效率** | 最低 | 最高 | 高（可并行） |
| **实际应用** | 较少 | 较少 | **最常用** |
| **噪声水平** | 无 | 最大 | 中等可控 |

---

## 关键概念知识点

### 学习率 (Learning Rate)
- **作用**：控制参数更新步长
- **问题**：
  - 过大 → 震荡/发散
  - 过小 → 收敛慢
- **策略**：学习率衰减、自适应学习率

### 收敛性
- **局部最优**：梯度为0的点
- **鞍点**：梯度为0但不是极值点
- **全局最优**：整个定义域的最小值

### 优化技巧
1. **动量 (Momentum)**
   - 引入历史梯度信息
   - 加速收敛，减少震荡
   - 公式：`v = βv + (1-β)∇J(θ)`, `θ = θ - η·v`

2. **自适应学习率方法**
   - **AdaGrad**：为每个参数调整学习率
   - **RMSProp**：解决AdaGrad学习率衰减过快
   - **Adam**：结合动量和自适应学习率（最常用）

### 挑战与问题
1. **梯度消失/爆炸**：深度网络中常见
2. **陷入局部最优**：非凸函数的挑战
3. **过拟合**：需要在损失函数中加入正则化

## 常见面试考点
- 批量大小对训练的影响
- 学习率选择策略
- SGD与Batch GD的对比
- 动量法的物理意义
- 如何判断梯度下降是否收敛
- 梯度下降与正规方程的比较（线性回归）
- 为什么需要学习率衰减

## 收敛条件
1. **梯度范数阈值**：`||∇J(θ)|| < ε`
2. **损失变化阈值**：`|J(θᵗ⁺¹) - J(θᵗ)| < ε`
3. **最大迭代次数**：达到预设迭代上限


# Section 3 Overfitting

![1764758193835](image/Revision-DL-Basis/1764758193835.png)

## 基本概念
- **过拟合**：模型在训练集上表现很好，但在新数据上表现较差
- **欠拟合**：模型在训练集和新数据上都表现不佳
- **泛化能力**：模型在新数据上的表现能力

## 图示理解
- **左图（欠拟合）**：模型过于简单（如神经元过少的神经网络）
- **右图（过拟合）**：模型过于复杂（如层数和神经元过多的神经网络）
- **核心问题**：难以找到恰到好处的模型复杂度

## 常见应对策略
**逐步优化法**：
1. 从**过于复杂**的模型开始
2. 应用正则化等技术防止过拟合
   
类比：购买衣服时不知道尺寸，先选大一点的，再通过修改（剪裁、缝纫）达到合身效果。

## 常用技术
- 正则化（L1/L2）
- Dropout（神经网络）
- 早停法（Early Stopping）
- 数据增强
- 简化模型结构
- 交叉验证


---

# Section 3 Weight Regularization

![1764758315510](image/Revision-DL-Basis/1764758315510.png)

## 核心观察
**大权重 → 过拟合**  
模型参数值过大时，往往表示模型过于复杂，对训练数据中的噪声也进行了学习。

## 基本思想
在损失函数中**添加权重大小的惩罚项**，限制权重增长，从而提高模型泛化能力。

## L1 正则化 (Lasso Regression)
**公式**：
\[
\min \frac{1}{M} \sum_i J(h_\theta(x_i), y_i) + \lambda \sum_j |\theta_j|
\]

**特点**：
- 产生**稀疏解**（许多权重变为0）
- 自动实现**特征选择**
- 适用于特征数量多、需要简化模型的情况

## L2 正则化 (Ridge Regression)
**公式**：
\[
\min \frac{1}{M} \sum_i J(h_\theta(x_i), y_i) + \lambda \sum_j \theta_j^2
\]

**特点**：
- 权重被**均匀缩小**，但不会归零
- **计算效率高**（有解析解）
- 适用于特征间存在相关性的情况

## 超参数 λ
- **正则化强度系数**
- λ越大 → 惩罚越重 → 模型越简单
- λ=0 → 退化为原始损失函数
- 需要通过交叉验证调整λ值

## 效果对比
| 方面 | L1正则化 | L2正则化 |
|------|---------|---------|
| 解的特性 | 稀疏，有零权重 | 稠密，权重均匀缩小 |
| 特征选择 | 支持 | 不支持 |
| 计算复杂度 | 较高 | 较低 |
| 适用场景 | 特征选择、模型简化 | 防止过拟合、数值稳定 |

## 实际应用
通常从L2正则化开始尝试，如果特征数量远大于样本数量或需要特征选择，则考虑L1正则化。

---

# Section 4 Dropout

![1764759252327](image/Revision-DL-Basis/1764759252327.png)
![1764759261177](image/Revision-DL-Basis/1764759261177.png)


### 1. 随机 Dropout（传统方法）
**核心思想**：在前向传播过程中，**随机**丢弃一部分神经元（通常50%）

**执行过程**：
- 训练时：每个神经元以概率 p 被暂时“丢弃”（输出置为0）
- 测试时：所有神经元都参与，但权重按 p 缩放

**优势**：
- 防止网络对**单个神经元**产生依赖
- 相当于训练多个子网络的**集成**
- 简单有效，广泛使用

**效果**：
- 增强模型泛化能力
- 减少神经元间的复杂共适应

### 2. 非随机 Dropout (meProp)
**核心思想**：基于**路径活跃度**有选择地丢弃神经元

**方法来源**：北京大学孙栩等人提出（2017年11月）

**执行过程**：
1. **训练阶段**：跟踪每条路径的“活跃度”（图中神经元深浅表示）
2. **简化阶段**：消除不活跃的神经元和连接
3. **迭代优化**：重复训练和简化过程

**技术细节**：
- **前向传播**：与标准网络相同
- **反向传播 (meProp)**：仅更新梯度最大的前 k% 权重（如top-1梯度）

**优势**：
- 保留**最重要的连接**
- 显著减少计算量
- 自动实现**模型压缩**

## 对比总结

| 特性 | 随机 Dropout | 非随机 Dropout (meProp) |
|------|-------------|------------------------|
| **丢弃方式** | 完全随机 | 基于路径活跃度（有选择） |
| **计算效率** | 较低（训练多个子网络） | 较高（聚焦重要连接） |
| **模型简化** | 不明显 | 显著（消除不活跃神经元） |
| **实现难度** | 简单 | 较复杂（需跟踪活跃度） |
| **主要目的** | 防止过拟合 | 模型压缩 + 防止过拟合 |

## 应用建议
- **标准场景**：使用随机Dropout（实现简单，效果稳定）
- **资源受限/移动端**：考虑meProp方法（模型更小，计算更快）
- **大规模网络**：可结合使用两种方法


---

# Section 5 Early Stopping
![1764759433375](image/Revision-DL-Basis/1764759433375.png)
![1764759554123](image/Revision-DL-Basis/1764759554123.png)

## 基本流程
1. **从复杂网络结构开始**（使用足够强大的模型）
2. **逐轮次评估模型**：绘制每个训练轮次（epoch）中训练集和验证集的误差变化曲线

## 关键观察
**验证集误差的变化规律**：
- 随着模型泛化能力提升，验证误差**逐渐下降**
- 到达一个**最低点**（最佳泛化点）
- 之后开始**上升**（出现过拟合）

这个最低点被称为 **“金发女孩点” (Goldilocks spot)** —— 正好停止欠拟合，开始出现过拟合的转折点。

---

## 基本思想
**在模型开始过拟合前停止训练**，不让网络有足够时间过拟合。

## 训练现象观察
随着训练轮数（epoch）增加：
1. **训练误差**：持续下降
2. **验证误差**：先下降后上升（出现过拟合拐点）


## 训练阶段分析
| 训练阶段 | 训练误差 | 验证误差 | 模型状态 |
|---------|---------|---------|---------|
| Epoch 1 | 大 | 大 | **欠拟合** |
| Epoch 20 | 小 | 小 | **刚刚好**（最佳泛化点） |
| Epoch 100 | 很小 | 中等 | **开始过拟合** |
| Epoch 600 | 极小 | 大 | **严重过拟合** |

## 关键概念
### 金发女孩点 (Goldilocks Spot)
- **最佳停止点**：验证误差达到最小值时
- **位置**：停止欠拟合和开始过拟合的边界
- **判断依据**：验证集误差停止改善并开始上升

## 实现方法
1. **监控验证集性能**：每个epoch后评估验证集
2. **设置耐心值**：允许性能短暂波动（如连续N次验证误差不改善）
3. **保存最佳模型**：记录验证集性能最好的模型权重
4. **提前终止**：当性能持续恶化时停止训练

## 优势与局限
**优势**：
- 简单有效，无需额外计算
- 自动确定最佳训练轮数
- 防止计算资源浪费

**局限**：
- 依赖验证集划分质量
- 可能过早停止（如果验证误差波动大）
- 需要额外保留验证集

