# DASC7606 Final - Transformer - summerized by hyperloop

# Section 0 Exam Scope

![](./Scope.png)

---

# Section 1 Transformer Training

![1764683422425](image/Revision/1764683422425.png)
![1764683430516](image/Revision/1764683430516.png)

## Transformer 翻译任务：Training vs Inference（核心知识点）

### 1) 训练阶段（Training / Teacher Forcing）
- **同时有输入序列 x 和目标序列 y（ground truth）**。
- **Encoder**：将输入 token → embedding + positional encoding → 多层 Encoder，得到输入的上下文表示（memory）。
- **Decoder 输入要“右移一位”(shifted right)**：在目标序列前加起始符 `<s>`，把 `(<s>, y1, y2, ...)` 作为 Decoder 的输入。
  - 含义：预测第 *i* 个目标词时，只能“看到”前面真实词 `y1...y{i-1}`（配合 **Masked Self-Attention** 防止偷看未来）。
- **Decoder 两类注意力**：
  - **Masked Self-Attention**：仅关注已生成/已给定的历史目标词。
  - **Cross-Attention（对 Encoder 输出做注意力）**：利用输入序列信息来生成目标词。
- **输出层**：Linear + Softmax → 得到每个位置的词概率分布。
- **损失函数**：将预测分布与真实目标词对比（常用 Cross-Entropy），用于反向传播更新参数。

### 2) 推理阶段（Inference / Autoregressive Decoding）
- **只有输入序列 x，没有目标序列 y**。
- **Encoder** 同训练：先把输入编码成 memory。
- **Decoder 从 `<s>` 开始自回归生成**：
  1. 第一个 decoder 输入是 `<s>`
  2. 输出词分布 → 选出 **第一个预测词**（如 argmax / sampling / beam search）
  3. 把该预测词拼到 decoder 输入末尾
  4. 重复以上过程，直到生成 `<eos>` 或达到最大长度
- 关键点：**推理时用“自己上一步的预测”作为下一步输入**；训练时通常用“真实词”（teacher forcing）。


### 3) 一句话对比
- **训练**：`decoder_input = <s> + y[:-1]`（右移，用真实历史词，算 loss）
- **推理**：`decoder_input = <s> + ŷ_generated_so_far`（右移，用模型自己生成的历史词，循环生成）



### 核心对比总结

| 方面                 | 训练 (Training)    | 推理 (Inference) |
| -------------------- | ------------------ | ---------------- |
| **解码器输入** | 真实目标序列       | 自生成序列       |
| **计算方式**   | 并行               | 顺序/自回归      |
| **参数更新**   | 有（梯度下降）     | 无               |
| **速度**       | 快（一次前向）     | 慢（逐词生成）   |
| **确定性**     | 使用完整标签       | 依赖生成策略     |
| **内存使用**   | 高（存储中间状态） | 较低             |

### Key Contrast Summary

| Aspect                      | Training                    | Inference                    |
| --------------------------- | --------------------------- | ---------------------------- |
| **Decoder Input**     | Ground truth target         | Self-generated tokens        |
| **Computation**       | Parallel                    | Sequential/Autoregressive    |
| **Parameter Updates** | Yes (gradient descent)      | No                           |
| **Speed**             | Fast (single forward)       | Slow (token-by-token)        |
| **Determinism**       | Uses full labels            | Depends on decoding strategy |
| **Memory Usage**      | High (stores intermediates) | Lower                        |

### 举例说明

**任务**：英译中 "Hello world" → "你好 世界"

**训练时**：

- 解码器输入：`[<sos>, "你", "好", "世", "界"]`（真实标签）
- 预测目标：`["你", "好", "世", "界", <eos>]`
- 一次计算所有位置的损失

**推理时**：

1. 输入 `<sos>` → 输出"你"
2. 输入 `<sos> "你"` → 输出"好"
3. 输入 `<sos> "你" "好"` → 输出"世"
4. 继续直到 `<eos>`

### 一句话总结

**训练**：使用正确答案并行学习模式；**推理**：基于已生成内容自回归预测下一个词。

**Training**: Learns patterns in parallel using correct answers; **Inference**: Predicts next token autoregressively based on generated content.

---

![1764683443107](image/Revision/1764683443107.png)

## Teacher Forcing 教学强制

### 是什么？

Teacher Forcing是一种在序列模型（如RNN、Transformer）训练时使用的技术，其中**解码器在每一步的输入是真实的上一目标词（ground truth）**，而不是模型自己上一时刻生成的词。

### 主要作用

1. **稳定训练**：防止错误累积和传播，加速收敛
2. **并行计算**：允许一次性计算所有时间步的损失（如Transformer训练）
3. **缓解曝光偏差**：训练时让模型看到“正确”的上下文
4. **改善梯度流动**：提供更清晰的训练信号

### 工作原理

- 训练时：使用真实目标序列作为输入
- 推理时：使用模型自己生成的序列作为输入
- 这种差异称为“训练-推理不匹配”

### What is it?

Teacher Forcing is a training technique used in sequence models (like RNNs, Transformers) where **the decoder receives the ground truth previous token as input at each step**, rather than the token generated by the model itself at the previous timestep.

### Main Functions

1. **Stabilizes Training**: Prevents error accumulation and propagation, speeds up convergence
2. **Enables Parallel Computation**: Allows computing loss for all timesteps at once (e.g., Transformer training)
3. **Mitigates Exposure Bias**: Lets model see "correct" context during training
4. **Improves Gradient Flow**: Provides clearer training signals

### How it Works

- During training: Uses ground truth target sequence as input
- During inference: Uses model's own generated sequence as input
- This discrepancy is called "training-inference mismatch"

### 核心对比

| 阶段                            | 输入来源     | 特点                           |
| ------------------------------- | ------------ | ------------------------------ |
| **Teacher Forcing训练**   | 真实目标序列 | 并行、稳定、快速收敛           |
| **非Teacher Forcing训练** | 自生成序列   | 顺序、模拟推理、更真实但更困难 |
| **推理阶段**              | 自生成序列   | 顺序、自回归、实际应用         |

### 一句话总结

**Teacher Forcing**：训练时给解码器“标准答案”作为输入，加速收敛但造成训练-推理不一致；是序列生成模型的关键训练技术。

**Teacher Forcing**: Feeds the decoder with "correct answers" during training to accelerate convergence, but creates training-inference discrepancy; a crucial training technique for sequence generation models.：

---



# Section 2 Transformer Architecture

![1764774558165](image/Revision-Transformer/1764774558165.png)
![1764774661135](image/Revision-Transformer/1764774661135.png)
![1764774675351](image/Revision-Transformer/1764774675351.png)
![1764774755458](image/Revision-Transformer/1764774755458.png)
![1764774771222](image/Revision-Transformer/1764774771222.png)
![1764774885009](image/Revision-Transformer/1764774885009.png)
![1764774893873](image/Revision-Transformer/1764774893873.png)

---

# Section 3 Attention

## Transformer 的三种注意力（3 Attention Types）

### 1) Encoder Self-Attention（编码器自注意力）
- **位置**：Encoder 每一层里
- **输入来源**：Q = K = V 都来自 *同一个* 输入序列表示（源句子 x 的 hidden states）
- **作用**：让源句子内部各 token 相互“看见”，建模长距离依赖与上下文语义
- **特点**：**不需要 mask**（因为源句子是完整已知的）



### 2) Decoder Masked Self-Attention（解码器带 Mask 的自注意力 / 因果注意力）
- **位置**：Decoder 每一层的第一块注意力
- **输入来源**：Q = K = V 来自 *Decoder 当前输入*（目标端已生成/已给定的 token 表示）
- **作用**：预测第 i 个词时，只能依赖 1..i-1 的历史目标词，避免“偷看未来”
- **关键机制**：使用 **look-ahead / causal mask**（上三角遮罩）
  - 训练时：配合 “shifted right” + teacher forcing
  - 推理时：天然就是自回归逐词生成

### 3) Encoder-Decoder Cross-Attention（交叉注意力 / 编解码注意力）
- **位置**：Decoder 每一层的第二块注意力（在 masked self-attn 之后）
- **输入来源**：
  - Q：来自 Decoder 当前层状态（目标端）
  - K, V：来自 Encoder 输出的 memory（源端）
- **作用**：让 Decoder 在生成每个目标词时，动态“对齐/关注”源句子中最相关的部分（类似软对齐）
- **直观理解**：**用目标端的问题 Q 去源端 memory 里检索信息（K/V）**

### 一句话总结
- Encoder self-attn：源句子内部建模  
- Decoder masked self-attn：目标句子“只能看过去”  
- Cross-attn：目标生成时从源句子表示里取信息
---
## Multi-head Attention（多头注意力）知识点总结
![1764683651993](image/Revision/1764683651993.png)


- **核心作用**：多头自注意力让模型对序列中不同位置赋予不同权重，从而更好地理解**语义与上下文**。
- **多头的做法**：把同一个 embedding 的特征维度切分/投影成多个子空间（多个 head），让每个 head **独立**做一次 attention，最后再拼接融合。
- **维度关系**：若 embedding 总维度为 \(E\)，head 数为 \(h\)，则每个 head 的维度（也可理解为每头的 Q/K/V 维度）为  
  \[
  d = \frac{E}{h}
  \]
  （通常要求 \(E\) 能被 \(h\) 整除）
- **为什么有效**：不同 head 可以并行学习不同类型的关联/关系模式，例如语法依赖、指代关系、属性（性别/数量/形容词/动作等）。
- **Q/K/V 分离**：每个 head 都有各自的线性映射生成 **Query（Q）/Key（K）/Value（V）**，用来计算注意力与聚合信息。
- **形状直观**：输入是 \(N \times E\)（N 个 token，每个 E 维），切到 h 个 head 后，每个 head 处理 \(N \times d\) 的表示。


### 多头注意力机制优点

1. **并行高效**：多头可独立并行计算，提升效率
2. **多维关注**：每个头关注不同语义维度（语法、语义、词性等）
3. **表示增强**：整合多头信息，得到更丰富的特征表示
4. **鲁棒性强**：多头互补，避免单一注意力模式的局限性
5. **复杂关系捕获**：同时处理局部和长距离依赖关系
6. **Parallel Efficiency**: Multiple heads compute independently in parallel
7. **Multi-dimensional Focus**: Each head attends to different semantic aspects
8. **Enhanced Representation**: Combines information from all heads for richer features
9. **Strong Robustness**: Heads complement each other, avoiding single-mode limitations
10. **Complex Relation Capture**: Handles both local and long-range dependencies simultaneously

---

## Transformer中的三种注意力掩码

![1764684200620](image/Revision/1764684200620.png)
![1764684264934](image/Revision/1764684264934.png)
![1764684286450](image/Revision/1764684286450.png)

### 1. Encoder Self-Attention Mask

**Purpose**: Masks padding tokens (`[PAD]`) to prevent attention to meaningless positions.
**Why**: Allows batch processing of sequences with different lengths.

### 2. Decoder Self-Attention Mask

**Purpose**: Masks future tokens to prevent the decoder from "peeking ahead".
**Why**: Ensures autoregressive generation during training and inference.

### 3. Encoder-Decoder Attention Mask

**Purpose**: Masks padding in encoder outputs when decoder attends to encoder.
**Why**: Ensures decoder only focuses on meaningful encoder representations.

---
# Section 4 Feedforward

![1764684482237](image/Revision/1764684482237.png)

## 前馈神经网络

在前馈神经网络中，信息仅向前传播。

1. **线性变换**
   全连接层，或密集层
2. **激活函数**
   引入非线性
3. **Dropout**
   防止过拟合
4. **层归一化**

- 稳定网络并加速训练

### **为什么前馈网络很重要？**

a) **学习复杂模式**：能够学习数据中的复杂模式和关系
b) **多功能性**：可用于各种任务
c) **与其他操作的兼容性**：结合其他操作的优点
d) **可扩展性和性能**：可独立并行化处理
e) **简单高效**：矩阵乘法 + 非线性激活
f) **支持深层网络**：通过批归一化和残差连接实现更深的网络架构

### 关键特点总结

| 组件     | 功能               | 重要性                   |
| -------- | ------------------ | ------------------------ |
| 线性层   | 特征变换与维度调整 | 基础计算单元             |
| 激活函数 | 引入非线性表达能力 | 使网络能拟合复杂函数     |
| Dropout  | 随机丢弃神经元     | 提高泛化能力，防止过拟合 |
| 层归一化 | 标准化层输出       | 稳定训练，加速收敛       |

### 在Transformer中的应用

- 每个编码器和解码器层都包含前馈网络
- 位置独立处理：每个位置的前馈计算相互独立
- 通常采用两层结构：扩展 → 激活 → 压缩

---

# Section 5 Advantages of Transformer

![1764684605003](image/Revision/1764684605003.png)

## Transformer 为何表现如此出色？

- LSTM 在2017年之前被广泛使用，但它无法很好地捕捉长期依赖关系，通常不超过7-15个词元。
- Google的Transformer（基于注意力机制的神经网络架构）专为序列到序列任务（如翻译）设计，相比传统的序列模型（如LSTM、RNN等）具有许多优势。

1. **可变大小的嵌入表示**
   根据输入大小而定
   （而不是RNN的固定大小状态向量），不会丢失信息
2. **词元之间的直接交互**
   （RNN或LSTM的序列处理依赖于词元之间的距离）
3. **可并行性**
   所有词元对的交互都可以并行计算

### 示意图说明

- 示例短语："A tall green creature"（一个高大的绿色生物）
- 在Transformer中，每个词元都可以直接与其他所有词元交互
- 通过多头注意力机制并行处理

| 对比维度             | Transformer          | RNN/LSTM                 |
| -------------------- | -------------------- | ------------------------ |
| **长距离依赖** | 直接连接，无距离衰减 | 依赖序列距离，信息衰减   |
| **计算方式**   | 并行处理所有位置     | 顺序处理，无法并行       |
| **信息保留**   | 通过注意力直接访问   | 通过隐藏状态传递，易丢失 |
| **训练效率**   | 高，支持大规模并行   | 低，受序列长度限制       |

### 核心优势总结

1. **全局上下文**：每个词元都能直接关注序列中所有其他词元
2. **计算效率**：摆脱了序列模型的顺序计算限制
3. **模型容量**：通过多头注意力学习复杂的依赖关系
4. **扩展性**：易于堆叠更多层，构建更深的网络
